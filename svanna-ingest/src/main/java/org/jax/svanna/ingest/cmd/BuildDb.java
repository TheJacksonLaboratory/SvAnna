package org.jax.svanna.ingest.cmd;


import com.zaxxer.hikari.HikariConfig;
import com.zaxxer.hikari.HikariDataSource;
import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;
import org.apache.commons.compress.archivers.zip.ZipFile;
import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;
import org.apache.commons.compress.compressors.gzip.GzipCompressorOutputStream;
import org.apache.commons.csv.CSVFormat;
import org.apache.commons.csv.CSVParser;
import org.apache.commons.csv.CSVRecord;
import org.apache.commons.io.IOUtils;
import org.flywaydb.core.Flyway;
import org.flywaydb.core.api.output.MigrateResult;
import org.jax.svanna.core.LogUtils;
import org.jax.svanna.core.hpo.TermPair;
import org.jax.svanna.db.IngestDao;
import org.jax.svanna.db.landscape.*;
import org.jax.svanna.db.phenotype.MicaDao;
import org.jax.svanna.ingest.Main;
import org.jax.svanna.ingest.config.*;
import org.jax.svanna.ingest.hpomap.HpoMapping;
import org.jax.svanna.ingest.hpomap.HpoTissueMapParser;
import org.jax.svanna.ingest.parse.IngestRecordParser;
import org.jax.svanna.ingest.parse.RepetitiveRegionParser;
import org.jax.svanna.ingest.parse.dosage.ClingenGeneCurationParser;
import org.jax.svanna.ingest.parse.dosage.ClingenRegionCurationParser;
import org.jax.svanna.ingest.parse.enhancer.fantom.FantomEnhancerParser;
import org.jax.svanna.ingest.parse.enhancer.vista.VistaEnhancerParser;
import org.jax.svanna.ingest.parse.population.DbsnpVcfParser;
import org.jax.svanna.ingest.parse.population.DgvFileParser;
import org.jax.svanna.ingest.parse.population.GnomadSvVcfParser;
import org.jax.svanna.ingest.parse.population.HgSvc2VcfParser;
import org.jax.svanna.ingest.parse.tad.McArthur2021TadBoundariesParser;
import org.jax.svanna.ingest.similarity.IcMicaCalculator;
import org.jax.svanna.model.landscape.dosage.DosageElement;
import org.jax.svanna.model.landscape.dosage.DosageSensitivity;
import org.jax.svanna.model.landscape.enhancer.Enhancer;
import org.jax.svanna.model.landscape.tad.TadBoundary;
import org.jax.svanna.model.landscape.variant.PopulationVariant;
import org.monarchinitiative.phenol.base.PhenolRuntimeException;
import org.monarchinitiative.phenol.ontology.data.TermId;
import org.monarchinitiative.svart.GenomicAssemblies;
import org.monarchinitiative.svart.GenomicAssembly;
import org.monarchinitiative.svart.GenomicRegion;
import org.monarchinitiative.svart.SequenceRole;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.boot.builder.SpringApplicationBuilder;
import org.springframework.boot.context.properties.EnableConfigurationProperties;
import org.springframework.context.ConfigurableApplicationContext;
import picocli.CommandLine;
import xyz.ielis.silent.genes.gencode.io.GencodeParser;
import xyz.ielis.silent.genes.gencode.model.Biotype;
import xyz.ielis.silent.genes.gencode.model.GencodeGene;
import xyz.ielis.silent.genes.io.GeneParser;
import xyz.ielis.silent.genes.io.GeneParserFactory;
import xyz.ielis.silent.genes.io.SerializationFormat;
import xyz.ielis.silent.genes.model.Gene;
import xyz.ielis.silent.genes.model.Located;

import javax.sql.DataSource;
import java.io.*;
import java.net.HttpURLConnection;
import java.net.URL;
import java.net.URLConnection;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.Callable;
import java.util.function.Predicate;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;
import java.util.stream.Stream;

@CommandLine.Command(name = "build-db",
        aliases = "B",
        header = "ingest the annotation files into H2 database",
        mixinStandardHelpOptions = true,
        version = Main.VERSION,
        usageHelpWidth = Main.WIDTH,
        footer = Main.FOOTER)
@EnableAutoConfiguration
@EnableConfigurationProperties(value = {
        IngestDbProperties.class,
        EnhancerProperties.class,
        VariantProperties.class,
        PhenotypeProperties.class,
        TadProperties.class,
        GeneDosageProperties.class,
        GeneProperties.class
})
public class BuildDb implements Callable<Integer> {

    private static final Logger LOGGER = LoggerFactory.getLogger(BuildDb.class);

    private static final String LOCATIONS = "classpath:db/migration";

    @CommandLine.Option(names = {"-o", "--overwrite"},
            description = "remove existing database (default: ${DEFAULT-VALUE})")
    public boolean overwrite = true;

    @CommandLine.Parameters(index = "0",
            paramLabel = "svanna-ingest-config.yml",
            description = "Path to configuration file generated by the `generate-config` command")
    public Path configFile;

    @CommandLine.Parameters(index = "1",
            description = "path to directory where the database will be built (default: ${DEFAULT-VALUE})")
    public Path buildDir = Path.of("data");

    public static DataSource initializeDataSource(Path dbPath) {
        DataSource dataSource = makeDataSourceAt(dbPath);

        int migrations = applyMigrations(dataSource);
        LOGGER.info("Applied {} migration(s)", migrations);
        return dataSource;
    }

    private static DataSource makeDataSourceAt(Path databasePath) {
        String absolutePath = databasePath.toFile().getAbsolutePath();
        if (absolutePath.endsWith(".mv.db"))
            absolutePath = absolutePath.substring(0, absolutePath.length() - 6);

        String jdbcUrl = String.format("jdbc:h2:file:%s", absolutePath);
        HikariConfig config = new HikariConfig();
        config.setUsername("sa");
        config.setPassword("sa");
        config.setDriverClassName("org.h2.Driver");
        config.setJdbcUrl(jdbcUrl);

        return new HikariDataSource(config);
    }

    private static int applyMigrations(DataSource dataSource) {
        Flyway flyway = Flyway.configure()
                .dataSource(dataSource)
                .locations(LOCATIONS)
                .load();
        MigrateResult migrate = flyway.migrate();
        return migrate.migrationsExecuted;
    }

    private static void downloadPhenotypeFiles(PhenotypeProperties properties, Path buildDir) throws IOException {
        List<String> fieldsToDownload = List.of(properties.hpoOboUrl(), properties.hpoAnnotationsUrl(),
                properties.mim2geneMedgenUrl(), properties.geneInfoUrl());
        for (String field : fieldsToDownload) {
            URL url = new URL(field);
            downloadUrl(url, buildDir);
        }
    }

    private static Path downloadAndPreprocessGenes(GeneProperties properties, GenomicAssembly assembly, Path buildDir, Path tmpDir) throws IOException {
        // download Gencode GTF
        URL url = new URL(properties.getGencodeGtfUrl());
        Path localGencodeGtfPath = downloadUrl(url, tmpDir);

        // transform the genes to silent gene format
        LOGGER.info("Reading Gencode GTF file at `{}`", localGencodeGtfPath.toAbsolutePath());
        GencodeParser parser = new GencodeParser(localGencodeGtfPath, assembly);
        List<GencodeGene> genes = parser.stream()
                .filter(geneIsCodingOrAtLeastOneTranscriptIsCoding())
                .collect(Collectors.toUnmodifiableList());
        LOGGER.info("Read {} genes", genes.size());

        // dump the transformed genes to compressed JSON file in the build directory
        GeneParserFactory parserFactory = GeneParserFactory.of(assembly);
        GeneParser jsonParser = parserFactory.forFormat(SerializationFormat.JSON);
        Path destination = buildDir.resolve("gencode.v38.genes.json.gz");
        LOGGER.info("Serializing the genes to `{}`", destination.toAbsolutePath());
        try (OutputStream os = new BufferedOutputStream(new GzipCompressorOutputStream(Files.newOutputStream(destination)))) {
            jsonParser.write(genes, os);
        }

        return destination;
    }

    private static Predicate<? super GencodeGene> geneIsCodingOrAtLeastOneTranscriptIsCoding() {
        // Gene is located on assembled molecule of the genomic assembly and gene is coding or at least one transcript is coding
        return gene -> gene.contig().sequenceRole() == SequenceRole.ASSEMBLED_MOLECULE
                && (gene.biotype() == Biotype.protein_coding || gene.transcripts().anyMatch(tx -> tx.biotype() == Biotype.protein_coding));
    }

    private static Path downloadLiftoverChain(IngestDbProperties properties, Path tmpDir) throws IOException {
        URL chainUrl = new URL(properties.hg19toHg38ChainUrl()); // download hg19 to hg38 liftover chain
        return downloadUrl(chainUrl, tmpDir);
    }

    private static void ingestEnhancers(EnhancerProperties properties,
                                        GenomicAssembly assembly,
                                        DataSource dataSource) throws IOException {
        Map<TermId, HpoMapping> uberonToHpoMap;
        try (InputStream is = BuildDb.class.getResourceAsStream("/uberon_tissue_to_hpo_top_level.csv")) {
            HpoTissueMapParser hpoTissueMapParser = new HpoTissueMapParser(is);
            uberonToHpoMap = hpoTissueMapParser.getOtherToHpoMap();
        }

        IngestRecordParser<? extends Enhancer> vistaParser = new VistaEnhancerParser(assembly, Path.of(properties.vista()), uberonToHpoMap);
        IngestDao<Enhancer> ingestDao = new EnhancerAnnotationDao(dataSource, assembly);
        int updated = ingestTrack(vistaParser, ingestDao);
        LOGGER.info("Ingest of Vista enhancers affected {} rows", updated);

        IngestRecordParser<? extends Enhancer> fantomParser = new FantomEnhancerParser(assembly, Path.of(properties.fantomMatrix()), Path.of(properties.fantomSample()), uberonToHpoMap);
        updated = ingestTrack(fantomParser, ingestDao);
        LOGGER.info("Ingest of FANTOM5 enhancers affected {} rows", updated);
    }

    private static void ingestPopulationVariants(VariantProperties properties, GenomicAssembly assembly, DataSource dataSource, Path tmpDir,
                                                 Path hg19Hg38chainPath) throws IOException {
        // DGV
        URL dgvUrl = new URL(properties.dgvUrl());
        Path dgvLocalPath = downloadUrl(dgvUrl, tmpDir);
        LOGGER.info("Ingesting DGV data");
        DbPopulationVariantDao ingestDao = new DbPopulationVariantDao(dataSource, assembly);
        int dgvUpdated = ingestTrack(new DgvFileParser(assembly, dgvLocalPath), ingestDao);
        LOGGER.info("DGV ingest updated {} rows", dgvUpdated);

        // GNOMAD SV
        URL gnomadUrl = new URL(properties.gnomadSvVcfUrl());
        Path gnomadLocalPath = downloadUrl(gnomadUrl, tmpDir);
        LOGGER.info("Ingesting GnomadSV data");
        IngestRecordParser<PopulationVariant> gnomadParser = new GnomadSvVcfParser(assembly, gnomadLocalPath, hg19Hg38chainPath);
        int gnomadUpdated = ingestTrack(gnomadParser, ingestDao);
        LOGGER.info("GnomadSV ingest updated {} rows", gnomadUpdated);

        // HGSVC2
        URL hgsvc2 = new URL(properties.hgsvc2VcfUrl());
        Path hgsvc2Path = downloadUrl(hgsvc2, tmpDir);
        LOGGER.info("Ingesting HGSVC2 data");
        IngestRecordParser<PopulationVariant> hgSvc2VcfParser = new HgSvc2VcfParser(assembly, hgsvc2Path);
        int hgsvc2Updated = ingestTrack(hgSvc2VcfParser, ingestDao);
        LOGGER.info("HGSVC2 ingest updated {} rows", hgsvc2Updated);

        // dbSNP
        URL dbsnp = new URL(properties.dbsnpVcfUrl());
        Path dbSnpPath = downloadUrl(dbsnp, tmpDir);
        LOGGER.info("Ingesting dbSNP data");
        IngestRecordParser<PopulationVariant> dbsnpVcfParser = new DbsnpVcfParser(assembly, dbSnpPath);
        int dbsnpUpdated = ingestTrack(dbsnpVcfParser, ingestDao);
        LOGGER.info("dbSNP ingest updated {} rows", dbsnpUpdated);
    }

    private static void ingestRepeats(IngestDbProperties properties, GenomicAssembly assembly, DataSource dataSource, Path tmpDir) throws IOException {
        URL repeatsUrl = new URL(properties.getRepetitiveRegionsUrl());
        Path repeatsLocalPath = downloadUrl(repeatsUrl, tmpDir);

        LOGGER.info("Ingesting repeats data");
        int repetitiveUpdated = ingestTrack(new RepetitiveRegionParser(assembly, repeatsLocalPath), new RepetitiveRegionDao(dataSource, assembly));
        LOGGER.info("Repeats ingest updated {} rows", repetitiveUpdated);
    }

    private static void ingestTads(TadProperties properties, GenomicAssembly assembly, DataSource dataSource, Path tmpDir, Path chain) throws IOException {
        // McArthur2021 supplement
        URL mcArthurSupplement = new URL(properties.mcArthur2021Supplement());
        Path localPath = downloadUrl(mcArthurSupplement, tmpDir);

        try (ZipFile zipFile = new ZipFile(localPath.toFile())) {
            // this is the single file from the entire ZIP that we're interested in
            String entryName = "emcarthur-TAD-stability-heritability-184f51a/data/boundariesByStability/100kbBookendBoundaries_mainText/100kbBookendBoundaries_byStability.bed";
            ZipArchiveEntry entry = zipFile.getEntry(entryName);
            InputStream is = zipFile.getInputStream(entry);
            IngestRecordParser<TadBoundary> parser = new McArthur2021TadBoundariesParser(assembly, is, chain);
            IngestDao<TadBoundary> dao = new TadBoundaryDao(dataSource, assembly);
            int updated = ingestTrack(parser, dao);
            LOGGER.info("Ingest of TAD boundaries affected {} rows", updated);
        }
    }

    private static void precomputeIcMica(Path buildDir, DataSource dataSource) {
        Path ontologyPath = buildDir.resolve("hp.obo");
        Path hpoaPath = buildDir.resolve("phenotype.hpoa");
        Map<TermPair, Double> similarityMap = IcMicaCalculator.precomputeIcMicaValues(ontologyPath, hpoaPath);

        MicaDao dao = new MicaDao(dataSource);
        similarityMap.forEach(dao::insertItem);
    }

    private static Map<TermId, GenomicRegion> readGeneRegions(Path gencodeJsonPath, GenomicAssembly assembly) throws IOException {
        GeneParserFactory factory = GeneParserFactory.of(assembly);
        GeneParser geneParser = factory.forFormat(SerializationFormat.JSON);

        LOGGER.info("Reading genes from `{}`", gencodeJsonPath);
        List<? extends Gene> genes;
        try (InputStream is = new BufferedInputStream(new GzipCompressorInputStream(Files.newInputStream(gencodeJsonPath)))) {
            genes = geneParser.read(is);
        }
        LOGGER.info("Read {} genes", genes.size());

        Map<TermId, GenomicRegion> regionsByHgncId = new HashMap<>(genes.size());
        for (Gene gene : genes) {
            Optional<String> hgncIdOptional = gene.id().hgncId();
            if (hgncIdOptional.isEmpty())
                continue;

            try {
                TermId hgncId = TermId.of(hgncIdOptional.get());
                regionsByHgncId.put(hgncId, gene.location());
            } catch (PhenolRuntimeException e) {
                LOGGER.warn("Invalid HGNC id `{}` in gene {}", hgncIdOptional.get(), gene);
            }
        }

        return regionsByHgncId;
    }

    private static void ingestGeneDosage(GeneDosageProperties properties,
                                         GenomicAssembly assembly,
                                         DataSource dataSource,
                                         Path tmpDir,
                                         Map<TermId, ? extends GenomicRegion> geneRegions) throws IOException {
        DosageElementDao dosageElementDao = new DosageElementDao(dataSource, assembly);

        // read NCBIGene to HGNC table
        Map<Integer, Integer> ncbiGeneToHgnc = parseNcbiToHgncTable(properties.getNcbiGeneToHgnc());

        // dosage sensitive genes
        URL geneUrl = new URL(properties.getGeneUrl());
        Path geneLocalPath = downloadUrl(geneUrl, tmpDir);
        ClingenGeneCurationParser geneParser = new ClingenGeneCurationParser(geneLocalPath, assembly, geneRegions, ncbiGeneToHgnc);
        try (Stream<? extends DosageElement> geneStream = geneParser.parse()) {
            int geneUpdated = geneStream
                    // only store the dosage sensitive genes and not necessarily all genes
                    .filter(dosageElement -> !DosageSensitivity.NONE.equals(dosageElement.dosageSensitivity()))
                    .mapToInt(dosageElementDao::insertItem)
                    .sum();
            LOGGER.info("Ingest of dosage sensitive genes affected {} rows", geneUpdated);
        }

        // dosage sensitive regions
        URL regionUrl = new URL(properties.getRegionUrl());
        Path regionLocalPath = downloadUrl(regionUrl, tmpDir);
        ClingenRegionCurationParser regionParser = new ClingenRegionCurationParser(regionLocalPath, assembly);
        try (Stream<? extends DosageElement> regionStream = regionParser.parse()) {
            int regionsUpdated = regionStream
                    // only store the dosage sensitive genes and not necessarily all genes
                    .filter(dosageElement -> !DosageSensitivity.NONE.equals(dosageElement.dosageSensitivity()))
                    .mapToInt(dosageElementDao::insertItem)
                    .sum();
            LOGGER.info("Ingest of dosage sensitive regions affected {} rows", regionsUpdated);
        }
    }

    private static Map<Integer, Integer> parseNcbiToHgncTable(String ncbiGeneToHgnc) throws IOException {
        Path tablePath = Path.of(ncbiGeneToHgnc);
        if (Files.notExists(tablePath)) {
            throw new IOException("Table for mapping NCBIGene to HGNC does not exist at `" + tablePath.toAbsolutePath() + "`");
        }

        Map<Integer, Integer> results = new HashMap<>();
        try (BufferedReader reader = Files.newBufferedReader(tablePath);
             CSVParser parser = CSVFormat.TDF.withFirstRecordAsHeader().parse(reader)) {
            Pattern hgncPattern = Pattern.compile("HGNC:(?<payload>\\d+)");
            // HGNC ID	NCBI gene ID	Approved symbol
            // HGNC:13666	8086	AAAS
            for (CSVRecord record : parser) {
                // parse NCBIGene. Should be a number, but may be missing.
                String ncbiGene = record.get("NCBI gene ID");
                if (ncbiGene.isBlank())
                    // missing NCBI gene ID for this gene
                    continue;

                int ncbiGeneId;
                try {
                    ncbiGeneId = Integer.parseInt(ncbiGene);
                } catch (NumberFormatException e) {
                    LOGGER.warn("Skipping non-numeric NCBIGene id `{}` on line #{}: `{}`", ncbiGene, record.getRecordNumber(), record);
                    continue;
                }

                // parse HGNC id
                Matcher hgncMatcher = hgncPattern.matcher(record.get("HGNC ID"));
                if (!hgncMatcher.matches()) {
                    LOGGER.warn("Skipping HGNC id `{}` on line #{}: `{}`", record.get("HGNC ID"), record.getRecordNumber(), record);
                    continue;
                }
                Integer hgncId = Integer.parseInt(hgncMatcher.group("payload"));

                // store the results
                results.put(ncbiGeneId, hgncId);
            }
        }
        return results;
    }

    private static <T extends Located> int ingestTrack(IngestRecordParser<? extends T> ingestRecordParser, IngestDao<? super T> ingestDao) throws IOException {
        return ingestRecordParser.parse()
                .mapToInt(ingestDao::insertItem)
                .sum();
    }

    private static Path downloadUrl(URL url, Path downloadDir) throws IOException {
        String file = url.getFile();
        String urlFileName = file.substring(file.lastIndexOf('/') + 1);
        Path localPath = downloadDir.resolve(urlFileName);
        LOGGER.info("Downloading data from `{}` to `{}`", url, localPath);
        downloadFile(url, localPath.toFile());
        return localPath;
    }

    private static void downloadFile(URL source, File target) throws IOException {
        if (target.isFile()) return;
        File parent = target.getParentFile();
        if (!parent.isDirectory() && !parent.mkdirs())
            throw new IOException("Unable to create parent directory `" + parent.getAbsolutePath() + "` for downloading `" + target.getAbsolutePath() + '`');

        URLConnection connection;
        if ("http".equals(source.getProtocol()))
            connection = openHttpConnectionHandlingRedirects(source);
        else
            connection = source.openConnection();

        try (BufferedInputStream is = new BufferedInputStream(connection.getInputStream())) {
            FileOutputStream os = new FileOutputStream(target);
            IOUtils.copyLarge(is, os);
        }
    }

    private static URLConnection openHttpConnectionHandlingRedirects(URL source) throws IOException {
        LogUtils.logDebug(LOGGER, "Opening HTTP connection to `{}`", source);
        HttpURLConnection connection = (HttpURLConnection) source.openConnection();
        int status = connection.getResponseCode();

        if (status != HttpURLConnection.HTTP_OK) {
            LogUtils.logDebug(LOGGER, "Received response `{}`", status);
            if (status == HttpURLConnection.HTTP_MOVED_PERM || status == HttpURLConnection.HTTP_MOVED_TEMP || status == HttpURLConnection.HTTP_SEE_OTHER) {
                String location = connection.getHeaderField("Location");
                LogUtils.logDebug(LOGGER, "Redirecting to `{}`", location);
                return new URL(location).openConnection();
            }
        }
        return connection;
    }

    @Override
    public Integer call() throws Exception {
        try (ConfigurableApplicationContext context = getContext()) {
            IngestDbProperties properties = context.getBean(IngestDbProperties.class);

            GenomicAssembly assembly = GenomicAssemblies.GRCh38p13();
            if (buildDir.toFile().exists()) {
                if (!buildDir.toFile().isDirectory()) {
                    if (LOGGER.isErrorEnabled()) LOGGER.error("Not a directory: `{}`", buildDir);
                    return 1;
                }
            } else {
                if (!buildDir.toFile().mkdirs()) {
                    if (LOGGER.isErrorEnabled()) LOGGER.error("Unable to create build directory");
                    return 1;
                }
            }

            Path dbPath = buildDir.resolve("svanna_db.mv.db");
            if (dbPath.toFile().isFile()) {
                if (overwrite) {
                    if (LOGGER.isInfoEnabled()) LOGGER.info("Removing the old database");
                    Files.delete(dbPath);
                } else {
                    LOGGER.info("Abort since the database already exists at `{}`. ", dbPath);
                    return 0;
                }
            }

            LOGGER.info("Creating database at `{}`", dbPath);
            DataSource dataSource = initializeDataSource(dbPath);

            Path tmpDir = buildDir.resolve("build");
            downloadPhenotypeFiles(properties.phenotype(), buildDir);
            Path gencodeJsonPath = downloadAndPreprocessGenes(properties.getGenes(), assembly, buildDir, tmpDir);
            ingestEnhancers(properties.enhancers(), assembly, dataSource);

            Path hg19ToHg38Chain = downloadLiftoverChain(properties, tmpDir);
            ingestPopulationVariants(properties.variants(), assembly, dataSource, tmpDir, hg19ToHg38Chain);
            ingestRepeats(properties, assembly, dataSource, tmpDir);
            ingestTads(properties.tad(), assembly, dataSource, tmpDir, hg19ToHg38Chain);
            precomputeIcMica(buildDir, dataSource);
            Map<TermId, GenomicRegion> geneMap = readGeneRegions(gencodeJsonPath, assembly);
            ingestGeneDosage(properties.getDosage(), assembly, dataSource, tmpDir, geneMap);

            LOGGER.info("The ingest is complete");
            return 0;
        }
    }

    protected ConfigurableApplicationContext getContext() {
        // bootstrap Spring application context
        return new SpringApplicationBuilder(BuildDb.class)
                .properties(Map.of("spring.config.location", configFile.toString()))
                .run();
    }

}
